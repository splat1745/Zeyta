# Zeyta â€” AI Assistant with Voice

A modular, local-first voice-based AI assistant powered by local LLMs, speech-to-text (Whisper/faster-whisper), and optimized text-to-speech with voice cloning support.

Table of contents
- Features
- Architecture & Project Structure
- Setup & Installation
- Configuration
- Running
- Troubleshooting
- Development & Contributing
- License & Credits

## Features

- ğŸ™ï¸ Voice Interaction: Speech-to-text using Whisper / faster-whisper and text-to-speech with voice cloning
- ğŸ§  Local LLM: Run on your hardware using Hugging Face Transformers
- ğŸ­ Voice Cloning: Multi-reference voice cloning via ChatterboxTTS
- âš¡ GPU Optimized: CUDA acceleration, memory pinning, and streaming
- ğŸ“ Conversation History: Maintains context across sessions
- ğŸ¯ Customizable Personality: Configure AI behavior via prompts
- ğŸ› ï¸ Standalone & testing apps: Terminal-based testing interfaces and an optional GUI app

## Architecture

Zeyta is organized modularly to separate concerns and make maintenance easier. Components are split into core logic, IO handlers, integrations, and testing tools.

## Project structure (overview)

```
zeyta/
â”œâ”€â”€ main.py                 # Main voice assistant entry point
â”œâ”€â”€ app.py                  # Standalone window / desktop app entry (optional)
â”œâ”€â”€ config.example.py       # Template configuration
â”œâ”€â”€ config.py               # (gitignored) Actual configuration - create from example
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ core/                   # Core logic
â”‚   â”œâ”€â”€ brain.py            # LLM interaction
â”‚   â”œâ”€â”€ context.py          # Conversation history management
â”‚   â””â”€â”€ controller.py       # Main orchestration loop
â”œâ”€â”€ IO/                     # Input/Output handlers
â”‚   â”œâ”€â”€ stt.py              # Speech-to-Text (Whisper / faster-whisper)
â”‚   â”œâ”€â”€ tts.py              # Text-to-Speech (Coqui / Piper)
â”‚   â”œâ”€â”€ coqui_backend.py    # Voice cloning backend (ChatterboxTTS)
â”‚   â”œâ”€â”€ mic_stream.py       # Microphone streaming utilities
â”‚   â””â”€â”€ vision.py           # Vision capabilities (future)
â”œâ”€â”€ integrations/           # Third-party integration modules (browser, smart home, etc.)
â”œâ”€â”€ utils/                  # Helper utilities (logger, profiler, tools)
â”œâ”€â”€ testing/                # Testing tools and apps
â”‚   â”œâ”€â”€ standalone_app.py
â”‚   â”œâ”€â”€ integrated_app.py
â”‚   â”œâ”€â”€ test_tts_clean.py
â”‚   â””â”€â”€ tts_server.py
â”œâ”€â”€ testing/outputs/        # Generated audio files during tests
â”œâ”€â”€ piper/                  # Optional Piper TTS backend files
â””â”€â”€ docs/                   # Documentation (APP_GUIDE, ARCHITECTURE, etc.)
```

## Prerequisites

- Python 3.11+
- FFmpeg installed on your system (for audio handling)
- CUDA-capable NVIDIA GPU recommended for best performance (8GB+ VRAM recommended)
- Optional: PyWebView for standalone window mode

## Installation

1. Clone the repository
```bash
git clone https://github.com/relfayoumi/Zeyta.git
cd Zeyta
```

2. Create and activate a virtual environment
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / macOS
source .venv/bin/activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

4. (Optional) Install PyTorch with CUDA (example for CUDA 12.1)
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

## Configuration

1. Copy the example configuration:
```bash
cp config.example.py config.py
```
2. Edit `config.py`:
- Set `LLM_MODEL_ID` to a Hugging Face model suitable for your hardware.
- Adjust `STT_MODEL_SIZE` (tiny, base, small, medium, large-v3).
- Choose `TTS_BACKEND`: `"coqui"` (voice cloning) or `"piper"` (fallback).
- Set `COQUI_REFERENCE_WAV` path for voice cloning reference files.
- Set `SYSTEM_PROMPT`, `GENERATION_ARGS`, and other options to tune behavior.

Note: `config.py` is intentionally excluded from git and stores personal settings.

## Running

- Run the interactive AI assistant (recommended):
```bash
python app.py
```

- Run the command-line assistant:
```bash
python main.py
```

- Run the integrated testing GUI (for model testing and development):
```bash
python testing/integrated_app.py
```

- Standalone terminal testing app:
```bash
python testing/standalone_app.py
```

- Test TTS optimization and voice cloning:
```bash
python testing/test_tts_clean.py --ref-dir IO/AudioRef_48kHz --expressive --temperature 0.75
```

- Run persistent TTS server:
```bash
python testing/tts_server.py
```

## Capabilities & Pipelines

Configurable pipelines include:
- Text Chat Only (LLM)
- Voice to Text (STT â†’ LLM)
- Voice to Voice (STT â†’ LLM â†’ TTS)
- Text to Voice (LLM â†’ TTS)

## Troubleshooting

- Import errors for faster-whisper:
```bash
pip install faster-whisper
```
- Import errors for chatterbox:
```bash
pip install chatterbox-tts
```
- CUDA Out of Memory:
  - Reduce model sizes in `config.py`
  - Use `STT_COMPUTE_TYPE = "int8"`
  - Close other GPU apps
  - Use smaller LLM models
- Audio issues:
  - Check microphone permissions
  - Verify `config.py` TTS backend setting
  - Inspect `testing/outputs/` for generated audio files

## Performance Tips

1. Use a GPU for inference when possible.
2. Balance model size vs. speedâ€”smaller models are faster with lower quality.
3. Tune generation args (temperature, top-p, max tokens).
4. Use 5â€“10 second reference clips for voice cloning.
5. Memory pinning and CUDA optimizations are used where available.

## Development & Contributing

Project philosophy:
- Modular design â€” each component is independent and testable.
- Local-first â€” runs on your hardware, no cloud dependencies.
- Optimized â€” GPU acceleration, efficient memory usage.
- Customizable â€” extensive configuration options.

Contributing:
1. Fork the repo
2. Create a feature branch
3. Implement and test your changes (use the standalone testing app)
4. Submit a pull request

## Documentation

See docs/ for more details:
- docs/APP_GUIDE.md
- docs/ARCHITECTURE.md
- docs/FEATURE_SHOWCASE.md
- docs/QUICK_REFERENCE.md
- docs/USAGE_EXAMPLES.md

## License

See the LICENSE file for details. Check individual dependency licenses for any external libraries used.

## Credits

Built with:
- Transformers (Hugging Face) â€” LLM inference
- Faster-Whisper â€” STT
- ChatterboxTTS â€” Voice cloning
- Piper TTS â€” Fast fallback TTS

---

This project emphasizes local, privacy-first AI assistance: all processing occurs on your hardware.
